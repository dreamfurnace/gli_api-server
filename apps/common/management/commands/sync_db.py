"""
Staging RDSì™€ ë¡œì»¬ PostgreSQL DB ë™ê¸°í™”

ì‚¬ìš©ë²•:
    # Stagingì—ì„œ ë°ì´í„° ë¤í”„í•˜ì—¬ S3ì— ì—…ë¡œë“œ
    DJANGO_ENV=staging python manage.py sync_db --dump

    # ë¡œì»¬ì—ì„œ S3ì—ì„œ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ë³µì›
    DJANGO_ENV=development python manage.py sync_db --load

    # ë°±ì—… ìƒì„± (ë¡œì»¬ ë°ì´í„° ë³´í˜¸)
    python manage.py sync_db --backup
"""

import os
import json
import gzip
from datetime import datetime
from pathlib import Path

from django.core.management.base import BaseCommand, CommandError
from django.core.management import call_command
from django.conf import settings
from django.db import connection

import boto3
from botocore.exceptions import ClientError


class Command(BaseCommand):
    help = 'Staging RDSì™€ ë¡œì»¬ DB ë™ê¸°í™”'

    def add_arguments(self, parser):
        parser.add_argument(
            '--dump',
            action='store_true',
            help='í˜„ì¬ DBë¥¼ ë¤í”„í•˜ì—¬ S3ì— ì—…ë¡œë“œ (stagingì—ì„œ ì‹¤í–‰)'
        )
        parser.add_argument(
            '--load',
            action='store_true',
            help='S3ì—ì„œ ë‹¤ìš´ë¡œë“œí•˜ì—¬ í˜„ì¬ DBì— ë³µì› (localì—ì„œ ì‹¤í–‰)'
        )
        parser.add_argument(
            '--backup',
            action='store_true',
            help='í˜„ì¬ ë¡œì»¬ DB ë°±ì—…'
        )
        parser.add_argument(
            '--s3-key',
            type=str,
            default='db-sync/latest-dump.json.gz',
            help='S3 ê°ì²´ í‚¤ (ê¸°ë³¸ê°’: db-sync/latest-dump.json.gz)'
        )
        parser.add_argument(
            '--exclude',
            type=str,
            nargs='+',
            default=['contenttypes', 'auth.permission', 'sessions.session'],
            help='ì œì™¸í•  ì•±/ëª¨ë¸ (ê¸°ë³¸ê°’: contenttypes, auth.permission, sessions.session)'
        )
        parser.add_argument(
            '--force',
            action='store_true',
            help='í™•ì¸ ì—†ì´ ì‹¤í–‰'
        )

    def handle(self, *args, **options):
        self.s3_key = options['s3_key']
        self.exclude = options['exclude']
        self.force = options['force']

        # S3 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.s3_client = boto3.client(
            's3',
            aws_access_key_id=settings.AWS_ACCESS_KEY_ID,
            aws_secret_access_key=settings.AWS_SECRET_ACCESS_KEY,
            region_name=settings.AWS_S3_REGION
        )
        self.bucket_name = settings.AWS_STORAGE_BUCKET_NAME

        if options['dump']:
            self.dump_and_upload()
        elif options['load']:
            self.download_and_load()
        elif options['backup']:
            self.backup_local_db()
        else:
            raise CommandError('--dump, --load, --backup ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì„¸ìš”')

    def dump_and_upload(self):
        """í˜„ì¬ DBë¥¼ ë¤í”„í•˜ì—¬ S3ì— ì—…ë¡œë“œ"""
        django_env = os.getenv('DJANGO_ENV', 'unknown')
        self.stdout.write(self.style.WARNING(f'ğŸ“¤ {django_env} DB ë¤í”„ ì‹œì‘...'))

        # ì„ì‹œ íŒŒì¼ ìƒì„±
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        temp_file = f'/tmp/db_dump_{timestamp}.json'

        try:
            # Django dumpdata ì‹¤í–‰
            self.stdout.write('ğŸ”„ ë°ì´í„° ì¶”ì¶œ ì¤‘...')
            with open(temp_file, 'w', encoding='utf-8') as f:
                call_command(
                    'dumpdata',
                    exclude=self.exclude,
                    indent=2,
                    stdout=f,
                    verbosity=0
                )

            # gzip ì••ì¶•
            self.stdout.write('ğŸ—œï¸  ì••ì¶• ì¤‘...')
            gz_file = f'{temp_file}.gz'
            with open(temp_file, 'rb') as f_in:
                with gzip.open(gz_file, 'wb') as f_out:
                    f_out.writelines(f_in)

            # íŒŒì¼ í¬ê¸° í™•ì¸
            original_size = os.path.getsize(temp_file)
            compressed_size = os.path.getsize(gz_file)
            self.stdout.write(
                f'ğŸ“Š ì›ë³¸: {original_size / 1024 / 1024:.2f} MB â†’ '
                f'ì••ì¶•: {compressed_size / 1024 / 1024:.2f} MB '
                f'({compressed_size / original_size * 100:.1f}%)'
            )

            # S3 ì—…ë¡œë“œ
            self.stdout.write(f'â˜ï¸  S3 ì—…ë¡œë“œ ì¤‘: s3://{self.bucket_name}/{self.s3_key}')
            with open(gz_file, 'rb') as f:
                self.s3_client.upload_fileobj(
                    f,
                    self.bucket_name,
                    self.s3_key,
                    ExtraArgs={
                        'Metadata': {
                            'source_env': django_env,
                            'dump_timestamp': timestamp,
                            'django_version': str(settings.VERSION) if hasattr(settings, 'VERSION') else 'unknown'
                        }
                    }
                )

            # íƒ€ì„ìŠ¤íƒ¬í”„ë³„ ë°±ì—…ë„ ì €ì¥
            backup_key = f'db-sync/backups/dump_{timestamp}.json.gz'
            with open(gz_file, 'rb') as f:
                self.s3_client.upload_fileobj(f, self.bucket_name, backup_key)

            self.stdout.write(self.style.SUCCESS(
                f'âœ… ë¤í”„ ì™„ë£Œ!\n'
                f'   - Latest: s3://{self.bucket_name}/{self.s3_key}\n'
                f'   - Backup: s3://{self.bucket_name}/{backup_key}'
            ))

        except Exception as e:
            raise CommandError(f'ë¤í”„ ì‹¤íŒ¨: {e}')
        finally:
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            for f in [temp_file, f'{temp_file}.gz']:
                if os.path.exists(f):
                    os.remove(f)

    def download_and_load(self):
        """S3ì—ì„œ ë‹¤ìš´ë¡œë“œí•˜ì—¬ í˜„ì¬ DBì— ë³µì›"""
        django_env = os.getenv('DJANGO_ENV', 'unknown')
        if django_env != 'development':
            raise CommandError('âš ï¸  ì´ ëª…ë ¹ì€ development í™˜ê²½ì—ì„œë§Œ ì‹¤í–‰í•˜ì„¸ìš”!')

        # í™•ì¸
        if not self.force:
            confirm = input(
                self.style.WARNING(
                    '\nâš ï¸  ê²½ê³ : í˜„ì¬ ë¡œì»¬ DBì˜ ëª¨ë“  ë°ì´í„°ê°€ ì‚­ì œë˜ê³  Staging ë°ì´í„°ë¡œ ëŒ€ì²´ë©ë‹ˆë‹¤.\n'
                    'ê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (yes/no): '
                )
            )
            if confirm.lower() != 'yes':
                self.stdout.write(self.style.ERROR('ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.'))
                return

        self.stdout.write(self.style.WARNING('ğŸ“¥ Staging DB ë‹¤ìš´ë¡œë“œ ë° ë³µì› ì‹œì‘...'))

        # ë¡œì»¬ DB ë°±ì—…
        self.stdout.write('ğŸ’¾ ë¡œì»¬ DB ë°±ì—… ì¤‘...')
        self.backup_local_db(auto=True)

        # ì„ì‹œ íŒŒì¼ ìƒì„±
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        gz_file = f'/tmp/db_restore_{timestamp}.json.gz'
        json_file = f'/tmp/db_restore_{timestamp}.json'

        try:
            # S3ì—ì„œ ë‹¤ìš´ë¡œë“œ
            self.stdout.write(f'â˜ï¸  S3 ë‹¤ìš´ë¡œë“œ ì¤‘: s3://{self.bucket_name}/{self.s3_key}')
            with open(gz_file, 'wb') as f:
                self.s3_client.download_fileobj(self.bucket_name, self.s3_key, f)

            # ì••ì¶• í•´ì œ
            self.stdout.write('ğŸ—œï¸  ì••ì¶• í•´ì œ ì¤‘...')
            with gzip.open(gz_file, 'rb') as f_in:
                with open(json_file, 'wb') as f_out:
                    f_out.write(f_in.read())

            # íŒŒì¼ í¬ê¸° í™•ì¸
            file_size = os.path.getsize(json_file)
            self.stdout.write(f'ğŸ“Š ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {file_size / 1024 / 1024:.2f} MB')

            # DB ì´ˆê¸°í™” (ê¸°ì¡´ ë°ì´í„° ì‚­ì œ)
            self.stdout.write(self.style.WARNING('ğŸ—‘ï¸  ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì¤‘...'))
            self.flush_database()

            # ë°ì´í„° ë¡œë“œ
            self.stdout.write('ğŸ”„ ë°ì´í„° ë³µì› ì¤‘...')
            call_command('loaddata', json_file, verbosity=2)

            self.stdout.write(self.style.SUCCESS('âœ… ë³µì› ì™„ë£Œ!'))

        except ClientError as e:
            if e.response['Error']['Code'] == 'NoSuchKey':
                raise CommandError(
                    f'S3ì— ë¤í”„ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: s3://{self.bucket_name}/{self.s3_key}\n'
                    'Staging í™˜ê²½ì—ì„œ ë¨¼ì € --dumpë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.'
                )
            raise CommandError(f'S3 ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}')
        except Exception as e:
            raise CommandError(f'ë³µì› ì‹¤íŒ¨: {e}')
        finally:
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            for f in [gz_file, json_file]:
                if os.path.exists(f):
                    os.remove(f)

    def backup_local_db(self, auto=False):
        """ë¡œì»¬ DB ë°±ì—…"""
        if not auto:
            self.stdout.write(self.style.WARNING('ğŸ’¾ ë¡œì»¬ DB ë°±ì—… ì‹œì‘...'))

        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_dir = Path(settings.BASE_DIR) / 'backups'
        backup_dir.mkdir(exist_ok=True)

        backup_file = backup_dir / f'local_backup_{timestamp}.json'

        try:
            with open(backup_file, 'w', encoding='utf-8') as f:
                call_command(
                    'dumpdata',
                    exclude=self.exclude,
                    indent=2,
                    stdout=f,
                    verbosity=0
                )

            file_size = os.path.getsize(backup_file)
            self.stdout.write(self.style.SUCCESS(
                f'âœ… ë°±ì—… ì™„ë£Œ: {backup_file} ({file_size / 1024 / 1024:.2f} MB)'
            ))

        except Exception as e:
            self.stdout.write(self.style.ERROR(f'ë°±ì—… ì‹¤íŒ¨: {e}'))

    def flush_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” (í…Œì´ë¸”ì€ ìœ ì§€, ë°ì´í„°ë§Œ ì‚­ì œ)"""
        with connection.cursor() as cursor:
            # ëª¨ë“  í…Œì´ë¸” ì°¾ê¸°
            cursor.execute("""
                SELECT tablename FROM pg_tables
                WHERE schemaname = 'public'
            """)
            tables = [row[0] for row in cursor.fetchall()]

            # ì™¸ë˜ í‚¤ ì œì•½ ì¡°ê±´ ì„ì‹œ ë¹„í™œì„±í™”
            for table in tables:
                cursor.execute(f'TRUNCATE TABLE "{table}" RESTART IDENTITY CASCADE')
